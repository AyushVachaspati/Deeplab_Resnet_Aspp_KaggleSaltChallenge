{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "jB2Ds8fYrGIk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dab23c6e-d9c2-41cb-dab0-14cb59ff9b7c"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json\tmnist_train.csv  saved_model\n",
            "mnist_test.csv\tsample_data\t saved_model.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z4fVmXkhrc8c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# downloading and loading images\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "def gdrive():\n",
        "    global drive\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcEP5zberfYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "17aa183b-7148-414e-fe13-55efbc36938d"
      },
      "cell_type": "code",
      "source": [
        "downloaded = drive.CreateFile({'id': \"1hydbOxr3GLmm5_zaGOfshvml5fYY_P8a\"})\n",
        "downloaded.GetContentFile(\"train.pkl\")\n",
        "print \"train\"\n",
        "gdrive()\n",
        "downloaded = drive.CreateFile({'id': \"1tXK20fHWydieLNhDiQo39ae0lfTxF3pa\"})\n",
        "downloaded.GetContentFile(\"validation.pkl\")\n",
        "print \"validation\"\n",
        "gdrive()\n",
        "downloaded = drive.CreateFile({'id': \"1Xp4xV6yAqYpTMfOQIQ4wkDk8sZT7fHeN\"})\n",
        "downloaded.GetContentFile(\"test.pkl\")\n",
        "print \"test\"\n",
        "gdrive()\n",
        "downloaded = drive.CreateFile({'id': \"1QDQxjVPxcDw7mrzMxep2OYCFkh8ElbSA\"})\n",
        "downloaded.GetContentFile(\"new.pkl\")\n",
        "print \"new\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "validation\n",
            "1.pkl  adc.json  sample_data  validation.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ssi_5fo-rnKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d81edc2d-e904-412c-b677-37ffd0f382f2"
      },
      "cell_type": "code",
      "source": [
        "train = True\n",
        "test = True\n",
        "batch_size = 36\n",
        "height = 101\n",
        "width = 101\n",
        "image_channels = 3\n",
        "num_classes= 2\n",
        "learn_rate = 1e-3\n",
        "\n",
        "image_batch = tf.placeholder(dtype = tf.float32,shape=[batch_size,height,width,image_channels])\n",
        "masks = tf.placeholder(dtype=tf.float32 ,shape=[batch_size,height,width,num_classes])\n",
        "is_training = tf.placeholder(dtype=tf.bool,shape=[])\n",
        "learning_rate = tf.placeholder(dtype=tf.float32,shape=[])\n",
        "\n",
        "#standardize the input\n",
        "images = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), image_batch)\n",
        "print images\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"map/TensorArrayStack/TensorArrayGatherV3:0\", shape=(36, 101, 101, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fFrdWDHc6H2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "65a09464-8c78-49d1-e6a8-78fb1bb10fb2"
      },
      "cell_type": "code",
      "source": [
        "#7x7 convolution stride 2 to downsample by factor 2\n",
        "conv1 = tf.layers.conv2d(images,\n",
        "                         filters=64,\n",
        "                         kernel_size = 7,\n",
        "                         strides=(2,2),\n",
        "                         padding='valid',\n",
        "                         activation=None,\n",
        "                         kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "# conv1= tf.layers.batch_normalization(conv1,training=True)\n",
        "print conv1\n",
        "conv1_bnr = tf.contrib.layers.batch_norm(conv1, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "print conv1_bnr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"conv2d/BiasAdd:0\", shape=(36, 48, 48, 64), dtype=float32)\n",
            "Tensor(\"BatchNorm/Relu:0\", shape=(36, 48, 48, 64), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U1_rwCwR59SR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "75657d43-ae54-4cf2-cf3c-9d0e31f71f5c"
      },
      "cell_type": "code",
      "source": [
        "conv2_blocks = 3\n",
        "unactivated = conv1\n",
        "activated = conv1_bnr\n",
        "for i in range(conv2_blocks):\n",
        "    with tf.variable_scope(\"block2_\"+str(i)):\n",
        "        conv_0 = tf.layers.conv2d(activated,\n",
        "                                   filters=128/2,\n",
        "                                   kernel_size = 1,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        conv_0 = tf.contrib.layers.batch_norm(conv_0, activation_fn=tf.nn.relu,is_training = is_training)      \n",
        "        print conv_0\n",
        "        \n",
        "        \n",
        "        conv_1 = tf.layers.conv2d(conv_0,\n",
        "                                   filters=128/2,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        conv_1 = tf.contrib.layers.batch_norm(conv_1, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "        print conv_1\n",
        "        \n",
        "        conv_2 = tf.layers.conv2d(conv_1,\n",
        "                                   filters=256,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        print conv_2\n",
        "        \n",
        "    if i==0:\n",
        "        concat = tf.layers.conv2d(unactivated,\n",
        "                                  filters=256,\n",
        "                                  kernel_size = 1,\n",
        "                                  strides=(1,1),\n",
        "                                  padding='same',\n",
        "                                  activation=None,\n",
        "                                  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        \n",
        "    else:\n",
        "        concat = unactivated\n",
        "        \n",
        "    output = tf.add(conv_2,concat)\n",
        "    unactivated = output\n",
        "    print unactivated\n",
        "    activated = tf.contrib.layers.batch_norm(output, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "    print activated"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"block2_0/BatchNorm/Relu:0\", shape=(36, 48, 48, 64), dtype=float32)\n",
            "Tensor(\"block2_0/BatchNorm_1/Relu:0\", shape=(36, 48, 48, 64), dtype=float32)\n",
            "Tensor(\"block2_0/conv2d_2/BiasAdd:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"Add:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"BatchNorm_1/Relu:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"block2_1/BatchNorm/Relu:0\", shape=(36, 48, 48, 64), dtype=float32)\n",
            "Tensor(\"block2_1/BatchNorm_1/Relu:0\", shape=(36, 48, 48, 64), dtype=float32)\n",
            "Tensor(\"block2_1/conv2d_2/BiasAdd:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"Add_1:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"BatchNorm_2/Relu:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"block2_2/BatchNorm/Relu:0\", shape=(36, 48, 48, 64), dtype=float32)\n",
            "Tensor(\"block2_2/BatchNorm_1/Relu:0\", shape=(36, 48, 48, 64), dtype=float32)\n",
            "Tensor(\"block2_2/conv2d_2/BiasAdd:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"Add_2:0\", shape=(36, 48, 48, 256), dtype=float32)\n",
            "Tensor(\"BatchNorm_3/Relu:0\", shape=(36, 48, 48, 256), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QPuKlbBQnaL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "ec17533b-8fb4-410b-80fa-e3839915a29a"
      },
      "cell_type": "code",
      "source": [
        "conv3_blocks = 3\n",
        "for i in range(conv3_blocks):\n",
        "    with tf.variable_scope(\"block3_\"+str(i)):\n",
        "        if i == 0:\n",
        "            conv_0 = tf.layers.conv2d(activated,\n",
        "                                       filters=256/2,\n",
        "                                       kernel_size = 1,\n",
        "                                       strides=(2,2),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "            conv_0 = tf.contrib.layers.batch_norm(conv_0, activation_fn=tf.nn.relu,is_training = is_training)      \n",
        "            print conv_0\n",
        "        else:\n",
        "            conv_0 = tf.layers.conv2d(activated,\n",
        "                                       filters=256/2,\n",
        "                                       kernel_size = 1,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "            conv_0 = tf.contrib.layers.batch_norm(conv_0, activation_fn=tf.nn.relu,is_training = is_training)      \n",
        "            print conv_0\n",
        "            \n",
        "        \n",
        "        \n",
        "        conv_1 = tf.layers.conv2d(conv_0,\n",
        "                                   filters=256/2,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        conv_1 = tf.contrib.layers.batch_norm(conv_1, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "        print conv_1\n",
        "        \n",
        "        conv_2 = tf.layers.conv2d(conv_1,\n",
        "                                   filters=512,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        print conv_2\n",
        "        \n",
        "    if i==0:\n",
        "        concat = tf.layers.conv2d(unactivated,\n",
        "                                  filters=512,\n",
        "                                  kernel_size = 1,\n",
        "                                  strides=(2,2),\n",
        "                                  padding='same',\n",
        "                                  activation=None,\n",
        "                                  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        \n",
        "    else:\n",
        "        concat = unactivated\n",
        "        \n",
        "    output = tf.add(conv_2,concat)\n",
        "    unactivated = output\n",
        "    print unactivated\n",
        "    activated = tf.contrib.layers.batch_norm(output, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "    print activated"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"block3_0/BatchNorm/Relu:0\", shape=(36, 24, 24, 128), dtype=float32)\n",
            "Tensor(\"block3_0/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 128), dtype=float32)\n",
            "Tensor(\"block3_0/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"Add_3:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"BatchNorm_4/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block3_1/BatchNorm/Relu:0\", shape=(36, 24, 24, 128), dtype=float32)\n",
            "Tensor(\"block3_1/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 128), dtype=float32)\n",
            "Tensor(\"block3_1/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"Add_4:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"BatchNorm_5/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block3_2/BatchNorm/Relu:0\", shape=(36, 24, 24, 128), dtype=float32)\n",
            "Tensor(\"block3_2/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 128), dtype=float32)\n",
            "Tensor(\"block3_2/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"Add_5:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"BatchNorm_6/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zeBRev-E5u2E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "46464cd5-1a4e-4057-f7a1-767b13c3d6dd"
      },
      "cell_type": "code",
      "source": [
        "conv4_blocks = 9\n",
        "for i in range(conv4_blocks):\n",
        "    with tf.variable_scope(\"block4_\"+str(i)):\n",
        "        if i == 0:\n",
        "            conv_0 = tf.layers.conv2d(activated,\n",
        "                                       filters=512/2,\n",
        "                                       kernel_size = 1,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "            conv_0 = tf.contrib.layers.batch_norm(conv_0, activation_fn=tf.nn.relu,is_training = is_training)      \n",
        "            print conv_0\n",
        "        else:\n",
        "            conv_0 = tf.layers.conv2d(activated,\n",
        "                                       filters=512/2,\n",
        "                                       kernel_size = 1,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "            conv_0 = tf.contrib.layers.batch_norm(conv_0, activation_fn=tf.nn.relu,is_training = is_training)      \n",
        "            print conv_0\n",
        "            \n",
        "        \n",
        "        \n",
        "        conv_1 = tf.layers.conv2d(conv_0,\n",
        "                                   filters=512/2,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                   dilation_rate = 2)\n",
        "        conv_1 = tf.contrib.layers.batch_norm(conv_1, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "        print conv_1\n",
        "        \n",
        "        conv_2 = tf.layers.conv2d(conv_1,\n",
        "                                   filters=1024,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        print conv_2\n",
        "        \n",
        "    if i==0:\n",
        "        concat = tf.layers.conv2d(unactivated,\n",
        "                                  filters=1024,\n",
        "                                  kernel_size = 1,\n",
        "                                  strides=(1,1),\n",
        "                                  padding='same',\n",
        "                                  activation=None,\n",
        "                                  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        \n",
        "    else:\n",
        "        concat = unactivated\n",
        "        \n",
        "    output = tf.add(conv_2,concat)\n",
        "    unactivated = output\n",
        "    print unactivated\n",
        "    activated = tf.contrib.layers.batch_norm(output, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "    print activated"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"block4_0/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_0/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_0/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_6:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_7/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_1/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_1/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_1/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_7:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_8/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_2/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_2/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_2/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_8:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_9/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_3/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_3/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_3/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_9:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_10/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_4/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_4/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_4/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_10:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_11/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_5/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_5/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_5/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_11:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_12/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_6/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_6/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_6/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_12:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_13/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_7/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_7/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_7/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_13:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_14/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"block4_8/BatchNorm/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_8/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 256), dtype=float32)\n",
            "Tensor(\"block4_8/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"Add_14:0\", shape=(36, 24, 24, 1024), dtype=float32)\n",
            "Tensor(\"BatchNorm_15/Relu:0\", shape=(36, 24, 24, 1024), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CF5izPiO73Fr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "1a7b70f1-e3fb-4669-99ad-ed9a1348bd25"
      },
      "cell_type": "code",
      "source": [
        "conv5_blocks = 3\n",
        "for i in range(conv5_blocks):\n",
        "    with tf.variable_scope(\"block5_\"+str(i)):\n",
        "        if i == 0:\n",
        "            conv_0 = tf.layers.conv2d(activated,\n",
        "                                       filters=1024/2,\n",
        "                                       kernel_size = 1,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "            conv_0 = tf.contrib.layers.batch_norm(conv_0, activation_fn=tf.nn.relu,is_training = is_training)      \n",
        "            print conv_0\n",
        "        else:\n",
        "            conv_0 = tf.layers.conv2d(activated,\n",
        "                                       filters=1024/2,\n",
        "                                       kernel_size = 1,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "            conv_0 = tf.contrib.layers.batch_norm(conv_0, activation_fn=tf.nn.relu,is_training = is_training)      \n",
        "            print conv_0\n",
        "            \n",
        "        \n",
        "        \n",
        "        conv_1 = tf.layers.conv2d(conv_0,\n",
        "                                   filters=1024/2,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                   dilation_rate = 2)\n",
        "        conv_1 = tf.contrib.layers.batch_norm(conv_1, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "        print conv_1\n",
        "        \n",
        "        conv_2 = tf.layers.conv2d(conv_1,\n",
        "                                   filters=2048,\n",
        "                                   kernel_size = 3,\n",
        "                                   strides=(1,1),\n",
        "                                   padding='same',\n",
        "                                   activation=None,\n",
        "                                   kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        print conv_2\n",
        "        \n",
        "    if i==0:\n",
        "        concat = tf.layers.conv2d(unactivated,\n",
        "                                  filters=2048,\n",
        "                                  kernel_size = 1,\n",
        "                                  strides=(1,1),\n",
        "                                  padding='same',\n",
        "                                  activation=None,\n",
        "                                  kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        \n",
        "    else:\n",
        "        concat = unactivated\n",
        "        \n",
        "    output = tf.add(conv_2,concat)\n",
        "    unactivated = output\n",
        "    print unactivated\n",
        "    activated = tf.contrib.layers.batch_norm(output, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "    print activated"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"block5_0/BatchNorm/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block5_0/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block5_0/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"Add_15:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"BatchNorm_16/Relu:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"block5_1/BatchNorm/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block5_1/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block5_1/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"Add_16:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"BatchNorm_17/Relu:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"block5_2/BatchNorm/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block5_2/BatchNorm_1/Relu:0\", shape=(36, 24, 24, 512), dtype=float32)\n",
            "Tensor(\"block5_2/conv2d_2/BiasAdd:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"Add_17:0\", shape=(36, 24, 24, 2048), dtype=float32)\n",
            "Tensor(\"BatchNorm_18/Relu:0\", shape=(36, 24, 24, 2048), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MX5nJ0PHQzkw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9a515dd6-ce46-49ff-f9bd-7290e99b3b8f"
      },
      "cell_type": "code",
      "source": [
        "#ASPP with bilinear interpolation\n",
        "with tf.variable_scope(\"prediction_and_interpolation\"):\n",
        "    aspp1 = tf.layers.conv2d(activated,\n",
        "                                       filters=2,\n",
        "                                       kernel_size = 3,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                       dilation_rate = 8)\n",
        "    aspp2 = tf.layers.conv2d(activated,\n",
        "                                       filters=2,\n",
        "                                       kernel_size = 3,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                       dilation_rate = 12)\n",
        "    aspp3 = tf.layers.conv2d(activated,\n",
        "                                       filters=2,\n",
        "                                       kernel_size = 3,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                       dilation_rate = 16)\n",
        "    aspp4 = tf.layers.conv2d(activated,\n",
        "                                       filters=2,\n",
        "                                       kernel_size = 3,\n",
        "                                       strides=(1,1),\n",
        "                                       padding='same',\n",
        "                                       activation=None,\n",
        "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                                       dilation_rate = 24)\n",
        "    \n",
        "    pred_mask =  tf.add_n([aspp1,aspp2,aspp3,aspp4])\n",
        "    \n",
        "    pred_mask = tf.contrib.layers.batch_norm(pred_mask, activation_fn=tf.nn.relu,is_training = is_training)\n",
        "    print pred_mask\n",
        "    \n",
        "    \n",
        "    pred_mask = tf.image.resize_bilinear(pred_mask,size = [101,101],align_corners=True)\n",
        "    print pred_mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"prediction_and_interpolation/BatchNorm/Relu:0\", shape=(36, 24, 24, 2), dtype=float32)\n",
            "Tensor(\"prediction_and_interpolation/ResizeBilinear:0\", shape=(36, 101, 101, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mdyCV5R7Rv3K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope(\"loss\"):\n",
        "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = masks, logits = pred_mask)\n",
        "    loss = tf.reduce_mean(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y14KcrphRyBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "minimize = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goxOy1frTylS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "init_op = tf.global_variables_initializer()\n",
        "sess.run(init_op)\n",
        "\n",
        "def save_model(sess):\n",
        "    !mkdir saved_model\n",
        "    saver = tf.train.Saver()\n",
        "    path = saver.save(sess,\"./saved_model/model.ckpt\")\n",
        "    print \"Model saved @ \" + path\n",
        "    return path\n",
        "  \n",
        "def restore_model(sess):\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, \"./saved_model/model.ckpt\")\n",
        "    \n",
        "def restore_model_from_drive(sess,file_id):\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(\"saved_model.tar.gz\")\n",
        "    !tar -xzvf saved_model.tar.gz\n",
        "    restore_model(sess)\n",
        "\n",
        "  \n",
        "  \n",
        "def upload_to_google_drive(filename,folder_id):\n",
        "    uploaded = False\n",
        "    file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
        "    try:\n",
        "        for file1 in file_list:\n",
        "            if (file1['title'] == filename) and (not uploaded):\n",
        "                file1.Delete()\n",
        "                f = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": folder_id, }]})\n",
        "                f.SetContentFile(filename)\n",
        "                f.Upload()\n",
        "                uploaded = True\n",
        "        if not uploaded:\n",
        "            f = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": folder_id, }]})\n",
        "            f.SetContentFile(filename)\n",
        "            f.Upload()\n",
        "            uploaded = True\n",
        "    except:\n",
        "        print \"Failed\"\n",
        "  \n",
        "    return f.get(\"id\")\n",
        "\n",
        "def get_one_hot_masks(masks):\n",
        "    temp = (np.invert(masks.astype(dtype=bool)).astype(dtype = int))\n",
        "    masks=np.expand_dims(masks,axis=3)\n",
        "    temp =np.expand_dims(temp,axis=3)\n",
        "    return np.concatenate([temp,masks],axis = 3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XU6tnxB3T9mO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_ious(test_images,true_masks, batch_size):\n",
        "    no_of_batches = len(true_masks)/batch_size\n",
        "    ptr = 0\n",
        "    ious = []\n",
        "    for i in range(no_of_batches):\n",
        "        x = test_images[ptr:ptr+batch_size]\n",
        "        y = get_one_hot_masks(true_masks[ptr:ptr+batch_size])\n",
        "        prediction = sess.run(pred_mask,{image_batch:x,masks:y,is_training: True})\n",
        "        prediction = np.argmax(prediction,axis=3)\n",
        "        true_mask_bool = true_masks[ptr:ptr+batch_size].astype(dtype=bool)\n",
        "        pred_mask_bool = prediction.astype(dtype=bool)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            temp = (true_mask_bool[i]*pred_mask_bool[i])\n",
        "            intersection = np.sum(temp)\n",
        "            temp = (true_mask_bool[i]+pred_mask_bool[i])\n",
        "            union = np.sum(temp)\n",
        "            \n",
        "            if union:\n",
        "                ious.append(float(intersection)/union)\n",
        "            else:\n",
        "                ious.append(1.0)\n",
        "                \n",
        "        ptr += batch_size\n",
        "   \n",
        "    return np.array(ious)\n",
        "\n",
        "def train_model(train_images,train_masks,no_of_batches):\n",
        "    total = 0\n",
        "    ptr = 0\n",
        "    print \"learn_rate\",learn_rate\n",
        "    for j in range(no_of_batches):\n",
        "        total+=1\n",
        "        if j%10 == 0:\n",
        "            print total\n",
        "        x = train_images[ptr:ptr+batch_size]\n",
        "        y = get_one_hot_masks(train_masks[ptr:ptr+batch_size])\n",
        "        \n",
        "        sess.run(minimize,{image_batch: x, masks: y,is_training: True,learning_rate:learn_rate})\n",
        "        ptr+= batch_size\n",
        "    return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JfF7lhR7UA5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2275
        },
        "outputId": "3a251569-69d3-4354-c6ba-4f6556e87626"
      },
      "cell_type": "code",
      "source": [
        "if train:\n",
        "    folder_id = \"1RTzLh1j5v2mHSQHDeJJZPHxWVfxW3xth\"  #kaggle deeplab 3 folder in google drive\n",
        "    filename = \"saved_model.tar.gz\"\n",
        "    curr_iou =  0\n",
        "    saved_iou= 0\n",
        "    epoch = 200\n",
        "    with open(\"validation.pkl\",\"r\") as f:\n",
        "        validation_images, validation_masks= pickle.load(f)\n",
        "        \n",
        "    with open(\"train.pkl\",\"r\") as f:\n",
        "        train_images , train_masks = pickle.load(f)\n",
        "    \n",
        "    for i in range(1,epoch+1):\n",
        "        gdrive()\n",
        "        print \"epoch------------>>>  \"+ str(i)\n",
        "        batches = 0\n",
        "        x = None\n",
        "        y = None\n",
        "        for k in range(1):\n",
        "            no_of_batches = len(train_images)/batch_size\n",
        "            batches = train_model(train_images,train_masks,no_of_batches)\n",
        "            print batches\n",
        "            x = train_images[:batch_size]\n",
        "            y = get_one_hot_masks(train_masks[:batch_size])\n",
        "            curr_iou = np.mean(get_ious(validation_images,validation_masks,batch_size))\n",
        "            print \"loss: \" , sess.run(loss,{image_batch: x, masks: y,is_training: True})\n",
        "            print \"mean iou = \" + str(curr_iou)\n",
        "            if curr_iou > saved_iou:\n",
        "                save_model(sess)\n",
        "                !tar -czvf saved_model.tar.gz saved_model\n",
        "                print upload_to_google_drive(filename,folder_id)\n",
        "                saved_iou= curr_iou\n",
        "            \n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch------------>>>  1\n",
            "learn_rate 1e-05\n",
            "1\n",
            "11\n",
            "21\n",
            "31\n",
            "41\n",
            "51\n",
            "61\n",
            "71\n",
            "81\n",
            "91\n",
            "100\n",
            "loss:  0.07412914\n",
            "mean iou = 0.792897898319838\n",
            "epoch------------>>>  2\n",
            "learn_rate 1e-05\n",
            "1\n",
            "11\n",
            "21\n",
            "31\n",
            "41\n",
            "51\n",
            "61\n",
            "71\n",
            "81\n",
            "91\n",
            "100\n",
            "loss:  0.07412495\n",
            "mean iou = 0.7932571848658423\n",
            "epoch------------>>>  3\n",
            "learn_rate 1e-05\n",
            "1\n",
            "11\n",
            "21\n",
            "31\n",
            "41\n",
            "51\n",
            "61\n",
            "71\n",
            "81\n",
            "91\n",
            "100\n",
            "loss:  0.074102364\n",
            "mean iou = 0.7933722146556325\n",
            "epoch------------>>>  4\n",
            "learn_rate 1e-05\n",
            "1\n",
            "11\n",
            "21\n",
            "31\n",
            "41\n",
            "51\n",
            "61\n",
            "71\n",
            "81\n",
            "91\n",
            "100\n",
            "loss:  0.07407546\n",
            "mean iou = 0.7934390827585509\n",
            "epoch------------>>>  5\n",
            "learn_rate 1e-05\n",
            "1\n",
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-820c4aa19a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mno_of_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_of_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1d3265e47769>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_images, train_masks, no_of_batches)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_one_hot_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mptr\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TLhiTSxPUKoH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def RLenc(img, order='F', format=True):\n",
        "    \"\"\"\n",
        "    img is binary mask image, shape (r,c)\n",
        "    order is down-then-right, i.e. Fortran\n",
        "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
        "\n",
        "    returns run length as an array or string (if format is True)\n",
        "    \"\"\"\n",
        "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
        "    runs = []  ## list of run lengths\n",
        "    r = 0  ## the current run length\n",
        "    pos = 1  ## count starts from 1 per WK\n",
        "    for c in bytes:\n",
        "        if (c == 0):\n",
        "            if r != 0:\n",
        "                runs.append((pos, r))\n",
        "                pos += r\n",
        "                r = 0\n",
        "            pos += 1\n",
        "        else:\n",
        "            r += 1\n",
        "\n",
        "    # if last run is unsaved (i.e. data ends with 1)\n",
        "    if r != 0:\n",
        "        runs.append((pos, r))\n",
        "        pos += r\n",
        "        r = 0\n",
        "\n",
        "    if format:\n",
        "        z = ''\n",
        "\n",
        "        for rr in runs:\n",
        "            z += '{} {} '.format(rr[0], rr[1])\n",
        "        return z[:-1]\n",
        "    else:\n",
        "        return runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-K2WEjwjUPzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "def add_to_submission(runs,image_ids):\n",
        "    with open('submission.csv', 'a') as csvFile:\n",
        "        writer=csv.writer(csvFile)\n",
        "        for i in range(len(image_ids)):\n",
        "            row = [image_ids[i],runs[i]]\n",
        "            writer.writerow(row)\n",
        "    csvFile.close()\n",
        "    print \"added\"\n",
        "            \n",
        "if test:\n",
        "    file_id =\"1S2NP8laV8soVXNFHi-ff-Fcs5WXa2FH5\"\n",
        "    restore_model_from_drive(sess,file_id)\n",
        "    with open(\"new.pkl\",\"r\") as f:\n",
        "        new_images, image_ids = pickle.load(f)\n",
        "    print image_ids[0]\n",
        "    print new_images.shape\n",
        "    predicted_masks = []\n",
        "    \n",
        "#     normalize new images. 3 channels seperately\n",
        "    img_a = new_images[:, :, :, 0]\n",
        "    img_b = new_images[:, :, :, 1]\n",
        "    img_c = new_images[:, :, :, 2]\n",
        "\n",
        "    # normalizing per channel data:\n",
        "    img_a = (img_a - np.min(img_a)).astype(dtype=float) / (np.max(img_a) - np.min(img_a))\n",
        "    img_b = (img_b - np.min(img_b)).astype(dtype=float) / (np.max(img_b) - np.min(img_b))\n",
        "    img_c = (img_c - np.min(img_c)).astype(dtype=float) / (np.max(img_c) - np.min(img_c))\n",
        "\n",
        "    # putting the 3 channels back together:\n",
        "    new_images = np.empty(new_images.shape, dtype=np.float32)\n",
        "    new_images[:, :, :, 0] = img_a\n",
        "    new_images[:, :, :, 1] = img_b\n",
        "    new_images[:, :, :, 2] = img_c\n",
        "\n",
        "    del img_a,img_b,img_c\n",
        "\n",
        "    \n",
        "\n",
        "    header = [\"id\",\"rle_mask\"]\n",
        "    \n",
        "    with open('submission.csv', 'w') as csvFile:\n",
        "        writer = csv.writer(csvFile)\n",
        "        writer.writerow(header)\n",
        "\n",
        "    csvFile.close()\n",
        "    \n",
        "    #get masks for the new images\n",
        "    ptr = 0 \n",
        "    no_of_batches = new_images.shape[0]/batch_size\n",
        "    \n",
        "    \n",
        "    for i in range(no_of_batches):\n",
        "        x = new_images[ptr:ptr+batch_size]        \n",
        "        predicted_masks = np.argmax(sess.run(pred_mask,{image_batch:x,is_training: True}),axis=3).astype(dtype=)\n",
        "        runs = []\n",
        "        for i in predicted_masks:\n",
        "            runs.append(RLenc(i))\n",
        "        add_to_submission(runs,image_ids[ptr:ptr+batch_size])\n",
        "        ptr+=batch_size\n",
        "\n",
        "        \n",
        "        \n",
        "    #upload to google drive\n",
        "    folder_id = \"1s0ALTUOhZXdE14DRGPBQ4N7QBOu6673S\" \n",
        "    upload_to_google_drive(\"submission.csv\",folder_id)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}